import os
import sys
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
import logging
from datetime import date, datetime
import yaml
import re
from urllib.parse import urlparse
from pathlib import Path

class NewsSpider(CrawlSpider):
    name = 'news'
    user_agent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"
    
    def __init__(self, config_file='', **kwargs):
        self.logger.info("Reading configuration  : %s", config_file)
        #read configuration 
        with open(config_file) as f:
            
            try:
                config = yaml.safe_load(f)
                self.allowed_domains = config['allowed_domains']
                self.start_urls = config['start_urls']
                self.sections_xpath = config['sections_xpath']
                self.section_name_xpath = config['section_name_xpath']
                self.sub_sections_xpath = config['sub_sections_xpath']
                self.sub_section_name_xpath = config['sub_section_name_xpath']
                self.articles_xpath = config['articles_xpath']
                self.articles_next_page_xpath = config['articles_next_page_xpath']
                self.article_title_xpath = config['article_title_xpath']
                self.article_summary_xpath = config['article_summary_xpath']
                self.article_author_xpath=  config['article_author_xpath']
                self.article_author_link_xpath=  config['article_author_link_xpath']
                self.article_date_time_xpath = config['article_date_time_xpath']  
                self.article_hashtag_xpath = config['article_hashtag_xpath']  
                self.article_text_xpath =  config['article_text_xpath']
                self.article_image_xpath = config['article_image_xpath']
               

            except yaml.YAMLError as exc:
                self.logger.error(exc)

            super().__init__(**kwargs) 


    ## Setting rule for which section, we do not want to scrap
    rules = (
        Rule(LinkExtractor(allow=r'img-container'), callback='parse', follow=True),
    )
    
    def urlcheck(self, url):
        return bool(urlparse(url).scheme) and urlparse(url).scheme.startswith('http')

    ## Get the link to each main-section from the front-page
    def parse(self, response):
        self.logger.debug("Parsing website  : %s", self.allowed_domains)

        sections = response.xpath(self.sections_xpath)
        self.logger.info("Section links  : %s", sections)

        for section in sections:
            section_part =  section.xpath(".//@href").get()
            section = response.urljoin(section_part)  
            self.logger.debug("Section link : %s", section)
            yield response.follow(url=section, callback = self.parse_section, dont_filter=True)


    ## Loop over each section
    def parse_section(self, response):
        self.logger.debug("Parsing section : %s" , response.url)

        section_name = response.xpath(self.section_name_xpath).get()
        if not section_name:
            section_name = response.url.rsplit('/', 1)[-1]        
        self.logger.debug("Section name : %s", section_name)

        if self.sub_sections_xpath:
            sub_sections = response.xpath(self.sub_sections_xpath)
            self.logger.info("Sub-section Links  : %s", sub_sections)
            for sub_section in sub_sections:
                sub_section_part =  sub_section.xpath(".//@href").get()
                sub_section = response.urljoin(sub_section_part)
                self.logger.debug("Sub-section link : %s", sub_section)
                
                if self.urlcheck(sub_section):
                    yield response.follow(url=sub_section, callback = self.parse_sub_section, 
                        meta={'section_name': section_name},dont_filter=True)

        else: #if no sub-section, go to article
            yield response.follow(url=response.url, callback = self.parse_sub_section, 
                meta={'section_name': section_name},dont_filter=True)


    ## Loop over each sub-section inside the section
    def parse_sub_section(self, response):
        self.logger.debug("Parsing sub-section : %s" , response.url)

        section_name = response.request.meta['section_name']
        sub_section_name = ""

        if self.sub_sections_xpath:
            sub_section_name = response.xpath(self.sub_section_name_xpath).get()
            if not sub_section_name:
                sub_section_name = urlparse(response.url).path,
            self.logger.debug("Sub-section name : %s", sub_section_name)      

        articles = response.xpath(self.articles_xpath)
        #self.logger.debug("Article Links  : %s", articles)

        for article in articles:
            article_part = article.xpath(".//@href").get()
            article = response.urljoin(article_part)
            self.logger.debug("Article link : %s", article)
            if self.urlcheck(article):
                yield response.follow(url=article, callback = self.parse_article, 
                meta={'section_name': section_name, 'sub_section_name': sub_section_name},dont_filter=True)
            else:
                self.logger.debug("Skipping url : %s" , article)

            if self.articles_next_page_xpath:
                articles_next_page = response.xpath(self.articles_next_page_xpath)
                next_page_part = articles_next_page.xpath(".//@href").get()
                next_page = response.urljoin(next_page_part)
                if next_page:
                    yield scrapy.Request(url=next_page, callback= self.parse_sub_section,
                    meta={'section_name': section_name, 'sub_section_name': sub_section_name},dont_filter=True)


    ## Extract the article details
    def parse_article(self, response):
        self.logger.debug("Parsing article : %s" , response.url)

        section_name = response.request.meta['section_name']
        sub_section_name = response.request.meta['sub_section_name']
        crawl_date = datetime.strftime(datetime.today(), "%Y-%m-%d %H%M%S")

        source = "scrapy"
        author_link = ""
        date_time = ""
        hashtag = ""
        summary = ""

        title = response.xpath(self.article_title_xpath).get()
        author = response.xpath(self.article_author_xpath).get()
        
        if self.article_author_link_xpath:
            author_link = response.xpath(self.article_author_link_xpath).xpath(".//@href").get()
        
        if self.article_date_time_xpath:
            date_time = response.xpath(self.article_date_time_xpath).get()

        text = ' '.join(response.xpath(self.article_text_xpath).getall())

        if self.article_hashtag_xpath:
            hashtag = response.xpath(self.article_hashtag_xpath)

        if self.article_summary_xpath:
            summary = ' '.join(response.xpath(self.article_summary_xpath).getall())

        if self.article_image_xpath:
            image_url = response.xpath(self.article_image_xpath).extract_first()
            print(image_url)
            
        user_agent = response.request.headers.get('User-Agent')
        content = response.headers.get('Content-Type')
        language = response.headers.get('Content-Language')
        length = response.headers.get('Content-Length')

        yield {
            'website': urlparse(response.url).netloc,
            'section':section_name,
            'sub_section':sub_section_name,
            'article_url': response.url,
            'title': title,
            'author':author,
            'author_link':author_link,
            'published_date': date_time,
            'hashtag': hashtag,
            'article_summary' : summary,
            'text': text,
            'image_url': image_url,
            'crawl_date': crawl_date,
            'user-agent': user_agent.decode('utf-8') if user_agent else "",
            'content': content.decode('utf-8') if content else "" ,
            'language': language.decode('utf-8') if language else "",
            'length': length.decode('utf-8') if length else "",
            'source': source
            }

# Run the Spider
if len(sys.argv) > 3 and sys.argv[3]:
    output_dir = sys.argv[3]
else:
    output_dir = "data"

configs = []
CONFIG_DIR = "src/news/spiders/config/"

#run config provided in input arguments
if len(sys.argv) > 2 and sys.argv[2]:
    config = sys.argv[2]
    configs.append(config)
else:
    #run all by default
    configs = os.listdir(CONFIG_DIR)

logging.info("Config files : %s" , configs)

time = datetime.strftime(datetime.today(), "%Y.%m.%d.%H%M%S")

process = CrawlerProcess(settings={
    'FEED_EXPORT_ENCODING':'utf-8',
    'FEED_FORMAT': 'json',
    'FEED_URI': output_dir + os.path.sep + time + '.json'})    

for config in configs:
    logging.info("Running Crawler Process: %s" , config)
    process.crawl(NewsSpider, config_file=os.path.join(CONFIG_DIR, config))

process.start()
